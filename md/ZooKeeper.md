# ZooKeeper

这是一个分布式协调服务框架，是Apache Hadoop 的一个子项目。

可以用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。

举个例子，分布式协调服务很难做好，特别容易出现竞争条件和死锁等错误，而使用 ZooKeeper 可以降低分布式应用从零开始实现协调服务的难度。

[ZooKeeper 官网文档](https://zookeeper.apache.org/doc/current/zookeeperOver.html)

## 核心概念

### 集群角色

- 负责处理客户端发送的读、写事务请求。这里的事务请求可以理解这个请求具有事务的 ACID 特性。
- 同步写事务请求给其他节点，且需要保证事务的顺序性。
- 状态为 LEADING。

Follower：

- 负责处理客户端发送的读请求
- 转发写事务请求给 Leader。
- 参与 Leader 的选举。
- 状态为 FOLLOWING。

Observer：

- 和 Follower 一样，唯一不同的是，不参与 Leader 的选举，且状态为 OBSERING。
- 可以用来线性扩展读的 QPS。

#### Leader 

状态为 LEADING 的节点，为客户端提供读写服务。

主导「过半写成功」策略，即将写事务请求同步给其他节点，且需要保证事务的顺序性。

这里的事务请求可以理解这个请求具有事务的 ACID 特性。

#### Follower

状态为 FOLLOWING 的节点，能提供读服务。

参与 Leader 选举，也参与「过半写成功」策略（将写事务请求转发给 Leader）。

#### Observer

状态为 OBSERING 的节点，能提供读服务。

不参与 Leader 选举，也不参与「过半写成功」策略。

可以用来线性扩展集群的 QPS 性能。

### Session（会话）

客户端会话

### Znode（数据节点）

在 ZooKeeper 中，节点分为两类，一种是构成集群的机器节点，另一种是数据模型中的数据单元，即数据节点

ZooKeeper 将所有数据存储在内存中，数据模型是一颗文件树

每个 Znode 都保存了节点自身的数据，并且可以实现数据的原子性读写，但每个 Znode 存储的数据很小，通常在1MB 以内。

#### Znode 类型

##### PERSISTENT - 持久化目录节点

除非主动进行移除，否则该节点将一直保存在 ZooKeeper 上

##### EPHEMERAL - 临时节点

该节点的生命周期与客户端绑定，当客户端会话失效时，该节点将被删除

##### PERSISTENT_SEQUENTIAL/EPHEMERAL_SEQUENTIAL - （持久化/临时）顺序节点

当这个节点被创建时，ZooKeeper 会自动在节点名后追加一个整型数字，该数字是一个由父节点维护的自增数

分为持久化节点和临时节点两种

### 版本

在 Znode 中维护了一个数据结构 Stat，记录了这个Znode的 version （当前 Znode 的版本）、cversion （当前 Znode 子节点的版本）和 aversion （当前 Znode 的 ACL 版本）

每次 Znode 的数据更改时，版本号都会增加。

### Watcher（事件监听器）

ZooKeeper 允许客户端注册一些 Watcher，去监听它关心的目录节点，而且当目录节点发生变化（如数据改变、被删除、子目录节点增加删除）时，ZooKeeper 会将事件通知客户端。

### ACL（访问控制列表 ，Access Control Lists ）策略

ZooKeeper 采用 ACL 来进行权限控制，类似于 UNIX 文件系统的权限控制。

##### 权限

**CREATE**：创建**子节点**的权限

**READ**：获取节点数据和子节点列表的权限

**WRITE**：更新节点数据的权限

**DELETE**：删除**子节点**的权限

**ADMIN**：设置节点 ACL 的权限

## 保证

### 1. 顺序一致性

ZooKeeper 是主从模型，因此所有的写操作都会在「主节点」上执行，即由单机来维护客户端指令的顺序与他们发送至 ZooKeeper 的顺序一致一致。

### 2. 原子性

类似事务的原子性，一条命令发送到 ZooKeeper 集群，要么执行成功要么失败，没有中间状态。

### 3. 单一系统映像

客户端将看到相同的服务视图，而不管它连接到集群中的哪一台 ZooKeeper 服务器。

### 4. 可靠性（持久性）

一旦应用更新，数据将被持久化，直到数据被再次更新。

对于该保证有两个推论：

1、如果客户端得到了成功的返回码，说明写入成功，数据被持久化，如果出现了通信错误，超时等一些故障，客户端将不知道更新是否已应用。我们采取措施尽量减少失败，但唯一的保证是只有成功的返回码。 （这在Paxos中称为单调性条件。）

2、如果客户端已经读取到了数据或者写入成功了数据，都不会因为zk的失败而导致回滚。

### 5. 及时性（最终一致性）

客户端看到的服务视图保证在特定时间范围内是最新的。

## 一致性协议

### 2PC (Two-Phase Commit)

二阶段提交将一个事务的处理过程分为**投票**和**执行**两个阶段，核心是对每个事务都采用**先尝试后提交**的处理方式

#### 角色

**协调者**：统一调度所有节点的执行逻辑，并最终决定这些参与者是否要把事务真正进行提交。

**参与者**：被调度的分布式节点

#### 概述

##### 第一阶段：提交事务请求

**1.事务询问**：「协调者」向所有「参与者」发送事务的内容，然后询问是否可以执行事务提交操作。并且等待「参与者」的响应

**2.执行事务**：各「参与者」执行事务操作，并将Undo和Redo信息计入事务日志中

**3.响应事务询问**：「参与者」执行事务操作，之后会向「协调者」返回是否成功执行了事务的操作

由此可见，阶段一只是查看「参与者」是否可以执行事务，并不是真正执行事务操作

##### 第二阶段：执行事务提交

「协调者」根据「参与者」的反馈情况，判断接下来进行什么操作：

**4.执行事务提交**：所有的「参与者」都向「协调者」反馈Yes响应

**5.中断事务**：任何一个「参与者」向「协调者」返回No响应，或者在等待超时之后，「协调者」没有接收到**所有「参与者」**的反馈响应

#### 优点

原理简单，实现方便

#### 缺点

##### 同步阻塞

指「协调者」要等**所有「参与者」**反馈yes响应之后才会进行发送提交请求
那么比如现在有三个「参与者」，两个已经反馈了yes，而第三个没有还没有反馈yes，此时就要一直等到第三个反馈完yes之后「协调者」才会发送提交请求

##### 单点问题

因为只有一个「协调者」，当这个「协调者」宕掉的话，就会出现问题

##### 数据不一致（脑裂）

在「协调者」向所有「参与者」发送Commit请求时，可能只发送到一部分的「参与者」那里，导致只有一部分「参与者」收到了Commit请求。

那么这种情况下，收到Commit请求的「参与者」执行事务提交，没收到的则没执行，因此出现了数据不一致

##### 太过保守

任意一个节点出现问题都会导致整个事务失败

### 3PC (Three-Phase Commit)

3PC 是在 2PC 的基础上，将 2PC 中的「提交事务请求」阶段一分为二，由此解决了 2PC 中的无限期等待问题

#### 概述

##### 阶段一：canCommit

**1.事务询问**：「协调者」向所有「参与者」发送一个包含事务内容的 canCommit 请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应

**2.响应事务询问**：「参与者」在接收到来自「协调者」的 canCommit 请求后，会根据自身判断是否可以顺利执行事务，并向「协调者」返回响应

反馈Yes响应的话，「参与者」还会进入 Ready 状态

##### 阶段二：preCommit

「协调者」根据「参与者」的反馈情况，决定是否可以进行事务的 preCommit 操作：

**3.执行事务预提交**：「参与者」反馈的都是Yes

**3.1 发出请求**：「协调者」向所有「参与者」节点发出 preCommit 的请求，并进入 Prepared 阶段
**3.2事务预提交**：「参与者」收到 preCommit 请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中
**3.3响应事务执行**：如果「参与者」成功执行了事务操作，就会反馈给「协调者」 Ack 响应，同时等待最终的指令：提交（Commit）或者中止（Abort）

**4.中断事务**：任何一个「参与者」向「协调者」返回No响应，或者在等待超时之后，「协调者」没有接收到**所有「参与者」**的反馈响应

##### 阶段三：doCommit

这个阶段才会真正提交事务

**5.执行提交**：「协调者」处于正常工作状态，并且收到所有「参与者」的 Ack 响应

**5.1 发送提交请求**：「协调者」从 Prepared 状态转换到 Commit 状态，并向所有「参与者」发送 doCommit 请求
**5.2 事务提交**：「参与者」收到 doCommit 请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源
**5.3 反馈事务提交结果**：「参与者」完成事务提交之后，向「协调者」发送 Ack 消息
**5.4 完成事务**：「协调者」接收到所有「参与者」反馈的 Ack 消息后，完成事务提交

#### 存在的问题

如果「协调者」出现问题，或者和「参与者」之间的网络出现故障，都会导致「参与者」无法及时接收到来自「协调者」的 doCommit 请求或者 abort 请求。

对于这种情况，「参与者」都会在等待超时之后，继续进行事务提交

#### 优点（相对于 2PC 的缺点）

1.3PC降低了「参与者」的阻塞范围（解决了同步阻塞的一部分）

如果有「参与者」出现问题，那么在阶段一就能反馈给「协调者」，而阶段一还没有进行事务的提交操作

2.并且在出现单点故障后继续达成一致（解决了单点问题，太过保守的一部分）

如果「协调者」出现问题，「参与者」在等待超时之后会继续进行事务提交

#### 缺点

「参与者」接收到 preCommit 消息后，如果出现**网络分区**，此时「协调者」和「参与者」无法进行正常的网络通信，那么该「参与者」在等待超时之后依然会进行事务的提交，导致数据不一致

### Paxos 算法

Paxos 算法是一种基于消息传递的且具有高容错性的一种算法，解决的问题是一个可能发生宕机或者网络异常的分布式系统如何就某个值（决议）达成一致。该算法前提是假设不存在拜占庭将军问题，即假设所有消息都是完整的，没有被篡改的。

#### 拜占庭将军问题

拜占庭帝国有许多支军队，不同军队的将军之间必须制订一个统一的行动计划，从而做出进攻或者撒退的决定；

同时，各个将军在地理上都是被分隔开来的，只能依靠军队的通讯员来进行通讯；

然而，在所有的通讯员中可能会存在叛徒，这些叛徒可以任意篡改消息，从而达到欺骗将军的目的。

#### 角色

**Proposer**：提出提案

**Acceptor**：对提案做出裁决

**Learner**：通过学习得到提案

为了避免单点故障，会有一个 Acceptor 集合

一个 Acceptor 必须批准它收到的第一个提案

Proposer 向该集合发送提案，Acceptor 集合中的每个成员都有可能同意该提案，并且每个 Acceptor 只能批准一次提案

当有一半以上的成员同意，则同意该提案

#### 原理

##### Prepare 阶段（生成提案）

1.Proposer 选择一个提案编号 Mn，然后向 Acceptor 集合发送编号为Mn 的 Prepare 请求

2.如果 Acceptor 收到一个编号为 Mn 的 Prepare 请求，且编号 Mn 大于 Acceptor 响应过的、所有 Prepare 请求中的最大编号，那么它将会向 Proposer 反馈「最大编号提案的 value 值」，同时 Acceptor 承诺不会再接受编号比 Mn 小的提案

##### Accept 阶段（批准提案）

1.如果 Proposer 收到了来自 Acceptor 集合的半数以上的响应，则会发出一个针对[Mn，Vn] 提案的 Accept 请求给 Acceptor 集合。

注意，Vn 为收到的所有响应中「最大编号提案的 value 值」，如果响应中不包含任何提案，那么它就是任意值。

2.如果 Acceptor 收到这个针对[Mn，Vn] 提案的 Accept 请求，只要 Acceptor 尚未对任何编号大于 Mn 的 Prepare请求做出响应，那么该 Acceptor 就可以通过这个提案

##### 获取提案

**方案1**：一旦 Acceptor 接受了一个提案，就将该提案发送给所有 Learner
优点：Learner 能快速获取被选定的提案
缺点：由于需要让每个 Acceptor 与所有 Learner 逐个进行一次通信，通信次数至少为二者个数乘积 `(M*N)`

**方案2**：一旦 Acceptor 接受了一个提案，就将该提案发送给主 Learner，主 Learner 再通知其他的 Learner
优点：大大减少了通信次数 `(M+N-1)`
缺点：单点问题，主 Learner 可能出现故障

**方案3**：一旦 Acceptor 接受了一个提案，就将该提案发送给一个特定的 Learner 集合，该集合中的每个 Learner 都可以再通知其他 Learner
优点：Learner 集合中的 Learner 个数越多，可靠性就越好
缺点：增加了网络通信的复杂度

##### 通过选取主 Proposer 保证算法的活性

比如，现在有两个 Proposer（Proposer_1、Proposer_2） 和一个 Acceptor

此时 Proposer_1 发出一个提案编号为 M1 的提案，而 Acceptor 当前没有接收到任何的提案，所以就会向 Proposer_1 反馈，保证接受 Proposer_1 提案的值，并且保证不再接受任何比编号 M1 小的提案

而当这个时候，就是说 Proposer_1 还没有设定它的值，Proposer_2 又发起了编号为 M2 的提案给 Acceptor，因为编号 M2 大于编号 M1，所以 Acceptor 会接受 Proposer_2 的提案，并且向 Proposer_2 保证不再接受编号比 M2 小的提案

之后 Proposer_1 把它设定的值反馈给 Acceptor 的时候，由于 Acceptor 已经向 Proposer_2 做出了新的保证，所以不再接受 M1 ，这个时候 Proposer_1 把提案的编号改为 M3，然后重复刚才的循环过程。会把 M2 拒绝，Proposer_2 又生成 M4 依此类推，产生死循环，以至于永远得不到一个确认的提案。

**对应优化**

为了避免死循环，必须选择一个主 Proposer，提案只能由主 Proposer 负责提出

#### 应用

Chubby（分布式锁服务，GFS中 master选举）

### ZAB （ZooKeeper Atomic Broadcast）协议

特别为 ZooKeeper 设计的、支持崩溃恢复的原子广播协议，是一种类似于 2PC 的协议。

通过 ZAB 协议，ZooKeeper 可以进行集群间主备节点的数据同步，保证数据的一致性。

[深入浅出 Zookeeper 中的 ZAB 协议](https://network.51cto.com/article/704705.html)

#### 协议具体内容

ZAB 协议包括两种基本模式：数据恢复模式和消息广播模式。

当 ZooKeeper 处于启动过程中，或者当 Leader 挂了时，亦或者集群中已经不存在过半的服务器与 Leader 保持正常通信时，ZAB 协议就会**进入数据恢复模式**并选举产生新的 Leader。

当选举产生了新的Leader，同事集群中已经有过半的机器与该 Leader 完成了状态同步之后，ZAB 协议就会**进入消息广播模式**。

当一台同样遵守 ZAB 协议的服务器**新加入集群**时，就会自觉进入数据恢复模式，即找到 Leader ，并与其进行数据同步，然后一起参与到消息广播流程中去。

##### 消息广播

在消息广播过程中，Leader 会为每个 Follower 都分配一个单独队列，然后将需要广播的 Proposal 依次放入这些 FIFO 队列中去。

每一个 Follower 在接收到一个 Proposal 之后，都会先以事务日志的形式写入到本地磁盘，并在成功写入后，反馈给 Leader 一个 Ack 响应。

当 Leader 收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有 Follower 以通知其进行事务提交，同时 Leader 自身也会完成对事务的提交。

每一个 Follower 在接收到一个 Commit 消息之后，都会完成对事务的提交。

##### ZooKeeper 选举

###### 1. 投票给自己

投票信息包含节点ID (SID) 和事务ID（ZXID）。

SID 是配置好的，而且是唯一的；ZXID 是唯一的递增编号。

###### 2.将投票信息发给集群中的其他节点

###### 3.判断

「节点X」判断另一个节点 -「节点Y」发来的投票**是否是本轮投票**，以及「节点Y」**是否处于 LOOKING 状态**

###### 4.PK

「节点X」会将自己的投票和别人的投票进行 PK

比较规则如下：

优先选取 ZXID 较大的投票；

如果 ZXID 相等，则优先选取 SID 较大的投票

###### 5.更新投票信息

然后将投票信息再次发送出去。而节点 B 不需要更新投票信息，但是下一轮还需要再次将投票发出去。

###### 6.统计投票

每一轮投票，都会统计每台节点收到的投票信息，判断是否有过半的节点收到了相同的投票信息。节点 A 和 节点 B 收到的投票信息都为(2，0)，且数量来说，大于一半节点的数量，所以将节点 B 选出来作为 Leader。

###### 7.更新节点状态：

节点 A 作为 Follower，更新状态为 FOLLOWING，节点 B 作为 Leader，更新状态为 LEADING。

##### 运行期间，Leader 宕机了怎么办？
